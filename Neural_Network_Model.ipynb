{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "bc_data = pd.read_csv('Data/loan_application_encoded.csv').fillna(0)\n",
    "bc_data = bc_data.drop(columns=[\n",
    "    'CODE_GENDER_XNA', \n",
    "    'NAME_EDUCATION_TYPE_Academic degree', \n",
    "    'NAME_INCOME_TYPE_Other', \n",
    "    'NAME_INCOME_TYPE_Unemployed', \n",
    "    'NAME_HOUSING_TYPE_Co-op apartment', \n",
    "    'NAME_HOUSING_TYPE_Office apartment',\n",
    "    'SK_ID_CURR',\n",
    "    'AMT_CREDIT_y'\n",
    "])\n",
    "X = bc_data.drop(columns=['TARGET'])\n",
    "y = bc_data['TARGET']\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the resampled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2)\n",
    "\n",
    "# Standardize the feature values \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the binary classification neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.PReLU(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32),\n",
    "    tf.keras.layers.PReLU(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(16, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=.01)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=1024, validation_split=0.5, callbacks=[early_stopping_callback], class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.9).astype(int) # Convert probabilities to binary predictions\n",
    "precision = precision_score(y_test, y_pred_binary, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_binary, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=['Actual Negative', 'Actual Positive'], \n",
    "                     columns=['Negative', 'Positive'])\n",
    "\n",
    "# Print the results\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Samples Trained: {len(X_train):,}')\n",
    "print(f'Samples Tested: {len(X_test):,}')\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming your test set is X_test and y_test\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"AUC: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier for feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "rf_data = pd.read_csv('Data/loan_application_encoded.csv').fillna(0)\n",
    "rf_data = rf_data.drop(columns=[\n",
    "    'CODE_GENDER_XNA', \n",
    "    'NAME_EDUCATION_TYPE_Academic degree', \n",
    "    'NAME_INCOME_TYPE_Other', \n",
    "    'NAME_INCOME_TYPE_Unemployed', \n",
    "    'NAME_HOUSING_TYPE_Co-op apartment', \n",
    "    'NAME_HOUSING_TYPE_Office apartment',\n",
    "    'SK_ID_CURR',\n",
    "    'AMT_CREDIT_y'\n",
    "])\n",
    "X = rf_data.drop(columns=['TARGET'])\n",
    "y = rf_data['TARGET']\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the resampled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3)\n",
    "\n",
    "# Standardize the feature values \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# *************************************************************\n",
    "\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "rf_accuracy = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Test Accuracy: {rf_accuracy:.4f}')\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns  \n",
    "\n",
    "# Sort the feature importances in descending order and get the indices\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances in a horizontal bar chart\n",
    "plt.figure(figsize=(10, 12))  # Adjust the figure size as needed\n",
    "plt.title('Feature Importance')\n",
    "plt.barh(range(X_train.shape[1]), importances[indices], align='center')\n",
    "plt.yticks(range(X_train.shape[1]), [feature_names[i] for i in indices])\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important at the top\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "221/221 [==============================] - 1s 4ms/step - loss: 0.1944 - accuracy: 0.9350 - val_loss: 0.1631 - val_accuracy: 0.9483\n",
      "Epoch 2/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1636 - accuracy: 0.9481 - val_loss: 0.1595 - val_accuracy: 0.9492\n",
      "Epoch 3/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1624 - accuracy: 0.9481 - val_loss: 0.1602 - val_accuracy: 0.9488\n",
      "Epoch 4/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1610 - accuracy: 0.9486 - val_loss: 0.1583 - val_accuracy: 0.9495\n",
      "Epoch 5/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1606 - accuracy: 0.9487 - val_loss: 0.1583 - val_accuracy: 0.9493\n",
      "Epoch 6/1000\n",
      "221/221 [==============================] - 1s 4ms/step - loss: 0.1602 - accuracy: 0.9488 - val_loss: 0.1581 - val_accuracy: 0.9494\n",
      "Epoch 7/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9490 - val_loss: 0.1580 - val_accuracy: 0.9493\n",
      "Epoch 8/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1595 - accuracy: 0.9488 - val_loss: 0.1605 - val_accuracy: 0.9483\n",
      "Epoch 9/1000\n",
      "221/221 [==============================] - 1s 4ms/step - loss: 0.1592 - accuracy: 0.9490 - val_loss: 0.1576 - val_accuracy: 0.9495\n",
      "Epoch 10/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1594 - accuracy: 0.9488 - val_loss: 0.1582 - val_accuracy: 0.9492\n",
      "Epoch 11/1000\n",
      "221/221 [==============================] - 1s 4ms/step - loss: 0.1595 - accuracy: 0.9487 - val_loss: 0.1586 - val_accuracy: 0.9491\n",
      "Epoch 12/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1592 - accuracy: 0.9490 - val_loss: 0.1594 - val_accuracy: 0.9490\n",
      "Epoch 13/1000\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1594 - accuracy: 0.9489 - val_loss: 0.1585 - val_accuracy: 0.9494\n",
      "Epoch 14/1000\n",
      "219/221 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.9489Restoring model weights from the end of the best epoch: 9.\n",
      "221/221 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9489 - val_loss: 0.1594 - val_accuracy: 0.9492\n",
      "Epoch 14: early stopping\n",
      "3534/3534 [==============================] - 1s 368us/step - loss: 0.1572 - accuracy: 0.9498\n",
      "3534/3534 [==============================] - 1s 314us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, PReLU, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "vs_data = pd.read_csv('Data/loan_application_encoded.csv').fillna(0)\n",
    "vs_data = vs_data.drop(columns=[\n",
    "    'CODE_GENDER_XNA', \n",
    "    'NAME_EDUCATION_TYPE_Academic degree', \n",
    "    'NAME_INCOME_TYPE_Other', \n",
    "    'NAME_INCOME_TYPE_Unemployed', \n",
    "    'NAME_HOUSING_TYPE_Co-op apartment', \n",
    "    'NAME_HOUSING_TYPE_Office apartment',\n",
    "    'SK_ID_CURR',\n",
    "    'AMT_CREDIT_y'\n",
    "])\n",
    "\n",
    "# Separate features and target\n",
    "X = vs_data.drop(columns=['TARGET'])\n",
    "y = vs_data['TARGET']\n",
    "\n",
    "# Store the loan amounts for later use\n",
    "loan_amounts = vs_data['AMT_CREDIT_x']\n",
    "\n",
    "# Combine features and loan amounts before resampling\n",
    "data_with_loan_amounts = X.copy()\n",
    "data_with_loan_amounts['LOAN_AMOUNT'] = loan_amounts\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(data_with_loan_amounts, y)\n",
    "\n",
    "# Now, X_resampled includes the resampled loan amounts\n",
    "loan_amounts_resampled = X_resampled['LOAN_AMOUNT']\n",
    "X_resampled = X_resampled.drop(columns=['LOAN_AMOUNT'])\n",
    "\n",
    "# Split the resampled data\n",
    "X_train, X_test, y_train, y_test, loan_amounts_train, loan_amounts_test = train_test_split(\n",
    "X_resampled, y_resampled, loan_amounts_resampled, test_size=0.2)\n",
    "\n",
    "# Standardize the feature values \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the binary classification neural network model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(64),\n",
    "    PReLU(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32),\n",
    "    PReLU(),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='sigmoid'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "adam_optimizer = Adam(learning_rate=.01)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=1024, validation_split=0.5, callbacks=[early_stopping_callback], class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.9).astype(int) # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred_binary, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_binary, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive'])\n",
    "\n",
    "# Financial Impact Calculation\n",
    "true_negatives_loan_amount = loan_amounts_test[(y_test == 0) & (y_pred_binary.flatten() == 0)].sum()\n",
    "false_negatives_loan_amount = loan_amounts_test[(y_test == 1) & (y_pred_binary.flatten() == 0)].sum()\n",
    "false_positives_loan_amount = loan_amounts_test[(y_test == 0) & (y_pred_binary.flatten() == 1)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1572\n",
      "Test Accuracy: 0.9498\n",
      "Precision: 0.9999\n",
      "Recall: 0.8984\n",
      "F1-Score: 0.9465\n",
      "\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               56801                   3\n",
      "Actual Positive                5716               50554\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print()\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Loans = $184,205,824,196\n",
      "Tested Loans Total Amount= $36,841,164,839\n",
      "Granted Loans Amount = $37,252,810,560\n",
      "Good Loans = $34,053,508,310    (91.4%)\n",
      "Bad Loans = $3,199,302,251    (8.6%)\n",
      "Missed Good Loans = $1,273,941\n",
      "\n",
      "Ratio of Bad Loans to Missed Good Loans = 2,511.3\n"
     ]
    }
   ],
   "source": [
    "all_loans = vs_data['AMT_CREDIT_x'].sum()\n",
    "test_pct_amt = all_loans * .2\n",
    "granted_loans = true_negatives_loan_amount+false_negatives_loan_amount\n",
    "missed_good_loans = false_positives_loan_amount\n",
    "\n",
    "print(\"All Loans = ${:,.0f}\".format(all_loans))\n",
    "print(\"Tested Loans Total Amount= ${:,.0f}\".format(test_pct_amt))\n",
    "print(\"Granted Loans Amount = ${:,.0f}\".format(granted_loans))\n",
    "print(\"Good Loans = ${:,.0f}\".format(true_negatives_loan_amount), \"   ({:.1f}%)\".format((true_negatives_loan_amount/granted_loans)*100))\n",
    "print(\"Bad Loans = ${:,.0f}\".format(false_negatives_loan_amount), \"   ({:.1f}%)\".format((false_negatives_loan_amount/granted_loans)*100))\n",
    "print(\"Missed Good Loans = ${:,.0f}\".format(missed_good_loans))\n",
    "print()\n",
    "print(\"Ratio of Bad Loans to Missed Good Loans = {:,.1f}\".format((false_negatives_loan_amount/missed_good_loans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THRESHOLD = .9\n",
    "Good Loans = $34,126,695,189    (91.3%)\n",
    "Bad Loans = $3,269,430,841    (8.7%)\n",
    "Missed Good Loans = $0\n",
    "Ratio of Bad Loans to Opportunity Cost = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THRESHOLD=.5\n",
    "Good Loans = $34,066,727,604    (91.5%)\n",
    "Bad Loans = $3,158,148,609    (8.5%)\n",
    "Missed Good Loans = $8,310,645\n",
    "Ratio of Bad Loans to Opportunity Cost = 380.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THRESHOLD=.3\n",
    "Good Loans = $33,960,204,603    (92.0%)\n",
    "Bad Loans = $2,964,625,763    (8.0%)\n",
    "Missed Good Loans = $273,406,572\n",
    "Ratio of Bad Loans to Missed Good Loans = 10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THRESHOLD=.2\n",
    "Good Loans = $32,413,849,016    (92.7%)\n",
    "Bad Loans = $2,553,196,626    (7.3%)\n",
    "Missed Good Loans = $1,729,664,811\n",
    "Ratio of Bad Loans to Missed Good Loans = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THRESHOLD=.175\n",
    "Good Loans = $31,749,593,998    (93.3%)\n",
    "Bad Loans = $2,297,873,785    (6.7%)\n",
    "Missed Good Loans = $2,373,905,250\n",
    "Ratio of Bad Loans to Missed Good Loans = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THRESHOLD=.15\n",
    "Good Loans = $30,236,415,988    (93.8%)\n",
    "Bad Loans = $2,007,389,410    (6.2%)\n",
    "Missed Good Loans = $3,737,143,102\n",
    "Ratio of Bad Loans to Missed Good Loans = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THRESHOLD=.1\n",
    "Good Loans = $25,457,557,820    (94.9%)\n",
    "Bad Loans = $1,381,609,020    (5.1%)\n",
    "Missed Good Loans = $8,680,486,666\n",
    "Ratio of Bad Loans to Missed Good Loans = 0.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
